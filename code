import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torch.nn.init
import os
import torch.nn as nn
import time
import matplotlib.pyplot as plt
import tonic.transforms as transforms
import tonic
import numpy as np
import snntorch as snn
from snntorch import surrogate
from snntorch import functional as SF
from snntorch import spikeplot as splt
from snntorch import utils
import torch.nn as nn
import os
from torch.utils.data import DataLoader, random_split
import torch
import logging

logger = logging.getLogger('simple_example')
logger.setLevel(logging.DEBUG)

# 콘솔 출력을 지정합니다

ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)

# 파일 출력을 지정합니다.

fh = logging.FileHandler(filename="nadam100000.log")
fh.setLevel([logging.INFO](http://logging.info/))

# add ch to logger

logger.addHandler(ch)
logger.addHandler(fh)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
sensor_size = tonic.datasets.NMNIST.sensor_size

# Denoise removes isolated, one-off events

# time_window

frame_transform = transforms.ToFrame(sensor_size=sensor_size, time_window=1)

frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000),
transforms.ToFrame(sensor_size=sensor_size,
time_window=100000)
])

trainset = tonic.datasets.NMNIST(save_to='/home/tmxk503/PycharmProjects/pythonProject1/data/NMNIST', transform=frame_transform, train=True)
testset = tonic.datasets.NMNIST(save_to='./home/tmxk503/PycharmProjects/pythonProject1/data/NMNIST', transform=frame_transform, train=False)

# 랜덤 시드 고정

torch.manual_seed(777)

# GPU 사용 가능일 경우 랜덤 시드 고정

if device == 'cuda':
torch.cuda.manual_seed_all(777)

#batch_size = 100
'''
mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정
train=True, # True를 지정하면 훈련 데이터로 다운로드
transform=transforms.ToTensor(), # 텐서로 변환
download=True)

mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정
train=False, # False를 지정하면 테스트 데이터로 다운로드
transform=transforms.ToTensor(), # 텐서로 변환
download=True)

data_loader = torch.utils.data.DataLoader(dataset=mnist_train,
batch_size=batch_size,
shuffle=True,
drop_last=True)
'''
batch_size = 32
dataset_size = len(trainset)
train_size = int(dataset_size * 0.9)
validation_size = int(dataset_size * 0.1)

trainset, valset = random_split(trainset, [train_size, validation_size])
print(len(valset))
print(len(trainset))
trainloader = DataLoader(trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)
valloader = DataLoader(valset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(), shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors())

spike_grad = surrogate.fast_sigmoid(slope=75)
beta = 0.5
'''
class CNN(torch.nn.Module):

```
def __init__(self):
    super(CNN, self).__init__()
    self.keep_prob = 0.5
    # L1 ImgIn shape=(?, 28, 28, 1)
    #    Conv     -> (?, 28, 28, 32)
    #    Pool     -> (?, 14, 14, 32)
    self.layer1 = torch.nn.Sequential(
        torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(kernel_size=2, stride=2))
    # L2 ImgIn shape=(?, 14, 14, 32)
    #    Conv      ->(?, 14, 14, 64)
    #    Pool      ->(?, 7, 7, 64)
    self.layer2 = torch.nn.Sequential(
        torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(kernel_size=2, stride=2))
    # L3 ImgIn shape=(?, 7, 7, 64)
    #    Conv      ->(?, 7, 7, 128)
    #    Pool      ->(?, 4, 4, 128)
    self.layer3 = torch.nn.Sequential(
        torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
        torch.nn.ReLU(),
        torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))

    # L4 FC 4x4x128 inputs -> 625 outputs
    self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)
    torch.nn.init.xavier_uniform_(self.fc1.weight)
    self.layer4 = torch.nn.Sequential(
        self.fc1,
        torch.nn.ReLU(),
        torch.nn.Dropout(p=1 - self.keep_prob))
    # L5 Final FC 625 inputs -> 10 outputs
    self.fc2 = torch.nn.Linear(625, 10, bias=True)
    torch.nn.init.xavier_uniform_(self.fc2.weight)

def forward(self, x):
    out = self.layer1(x)
    out = self.layer2(out)
    out = self.layer3(out)
    out = out.view(out.size(0), -1)   # Flatten them for FC
    out = self.layer4(out)
    out = self.fc2(out)
    return out

```

'''

class CNN(torch.nn.Module):

```
def __init__(self):
    super(CNN, self).__init__()
    self.keep_prob = 0.5
    self.layer1 = torch.nn.Sequential(
        nn.Conv2d(2, 12, 5),
        nn.MaxPool2d(2),
        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True))

    self.layer2 = torch.nn.Sequential(
        nn.Conv2d(12, 32, 5),
        nn.MaxPool2d(2),
        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True))

    # L4 FC 4x4x128 inputs -> 625 outputs

    self.layer4 = torch.nn.Sequential(
        nn.Flatten(),
        nn.Linear(32 * 5 * 5, 10),
        snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True))
    # L5 Final FC 625 inputs -> 10 outputs

def forward(self, data):
    spk_rec = []
    layer1_rec = []
    layer2_rec = []
    utils.reset(self.layer1)  # resets hidden states for all LIF neurons in net
    utils.reset(self.layer2)
    utils.reset(self.layer4)

    for step in range(data.size(1)):  # data.size(0) = number of time steps
        input_torch = data[:, step, :, :, :]
        input_torch = input_torch.cuda()
        #print(input_torch)
        out = self.layer1(input_torch)
        out1 = out

        out = self.layer2(out)
        out2 = out
        out, mem = self.layer4(out)

        spk_rec.append(out)

        layer1_rec.append(out1)
        layer2_rec.append(out2)

    return torch.stack(spk_rec), torch.stack(layer1_rec), torch.stack(layer2_rec)

```

# CNN 모델 정의

#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"] = '1, 2, 3'
model = CNN().to(device)
optimizer = torch.optim.NAdam(model.parameters(), lr=0.005, betas=(0.9, 0.999))
loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)
#model = nn.DataParallel(model)

total_batch = len(trainloader)
print('총 배치의 수 : {}'.format(total_batch))
loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)
num_epochs = 15
loss_hist = []
acc_hist = []
v_acc_hist = []
t_spk_rec_sum = []
start = time.time()
val_cnt = 0
v_acc_sum= 0
avg_loss = 0
index = 0
#################################################

# for epoch in range(training_epochs):

# avg_cost = 0

# 

# for X, Y in trainloader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y느 ㄴ레이블.

# # image is already size of (28x28), no reshape

# # label is not one-hot encoded

# X = [X.to](http://x.to/)(device)

# Y = [Y.to](http://y.to/)(device)

# 

# optimizer.zero_grad()

# spk_rec, h1, h2 = model(X)

# cost = loss_fn(spk_rec, Y)

# cost.backward()

# optimizer.step()

# 

# avg_cost += cost / total_batch

# acc = SF.accuracy_rate(spk_rec, Y)

# 

# print(f"Train Accuracy: {acc * 100:.2f}%")

# print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))

for epoch in range(num_epochs):
torch.save(model.state_dict(), '/home/tmxk503/PycharmProjects/pythonProject1/tw/Nadam-100000-0.5.pt')
for i, (data, targets) in enumerate(iter(trainloader)):
data = data.cuda()
targets = targets.cuda()

```
    model.train()
    spk_rec, h1, h2 = model( data)
    #print(spk_rec.shape)
    loss_val = loss_fn(spk_rec, targets)
    avg_loss += loss_val.item()
    # Gradient calculation + weight update
    optimizer.zero_grad()

    loss_val.backward()
    optimizer.step()
    #print(spk_rec.shape)
    # Store loss history for future plotting
    loss_hist.append(loss_val.item())
    val_cnt = val_cnt+1

    if val_cnt == len(trainloader)/2-1:
        val_cnt=0

        for ii, (v_data, v_targets) in enumerate(iter(valloader)):
            v_data = v_data.to(device)
            v_targets = v_targets.to(device)

            v_spk_rec, h1, h2 = model(v_data)
            # print(t_spk_rec.shape)
            v_acc = SF.accuracy_rate(v_spk_rec, v_targets)
            if ii == 0:
                v_acc_sum = v_acc
                cnt = 1

            else:
                v_acc_sum += v_acc
                cnt += 1
        plt.plot(acc_hist)
        plt.plot(v_acc_hist)
        plt.legend(['train accuracy', 'validation accuracy'])
        plt.title("Train, Validation Accuracy-Nadam100000-0.5")
        plt.xlabel("Iteration")
        plt.ylabel("Accuracy")
        # plt.show()
        plt.savefig('Nadam100000-0.5.png')
        plt.clf()
        v_acc_sum = v_acc_sum/cnt

        avg_loss = avg_loss / (len(trainloader) / 2)
        print('average loss while half epoch', avg_loss)
        if avg_loss <= 0.5:
            index = 1
            break
        else:
            avg_loss = 0
            index = 0

    print('Nadam100000-0.5')
    print("time :", time.time() - start,"sec")
    print(f"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}")

    acc = SF.accuracy_rate(spk_rec, targets)
    acc_hist.append(acc)
    v_acc_hist.append(v_acc_sum)
    print(f"Train Accuracy: {acc * 100:.2f}%")
    print(f"Validation Accuracy: {v_acc_sum * 100:.2f}%\\n")
    if index == 1:
        break
if index == 1:
    break

```

# 학습을 진행하지 않을 것이므로 torch.no_grad()

'''
with torch.no_grad():
X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)
Y_test = mnist_test.test_labels.to(device)

```
prediction = model(X_test)
correct_prediction = torch.argmax(prediction, 1) == Y_test
accuracy = correct_prediction.float().mean()
print('Accuracy:', accuracy.item())

```

'''
